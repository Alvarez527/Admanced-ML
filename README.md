# ğŸ§  Deep Learning Foundations â€” From Fully Connected Nets to Transformers

Welcome to the **Deep Learning Foundations** repository! This curated collection of Jupyter notebooks guides you through key building blocks of modern artificial intelligence â€” starting from fully connected networks and progressing to transformers for natural language tasks.

Whether you're just starting out or aiming to deepen your understanding of practical implementations in deep learning, this repository provides a hands-on, code-first approach supported by theoretical insights.

---

## ğŸ“š Learning Path & Notebook Guide

### ğŸ”¹ Module 1: Foundations of Fully Connected Networks

- `1a_Multilayer_Fully_Connected_Numpy.ipynb`  
  â¤ Build a fully connected neural network from scratch using NumPy. Learn the mathematics of forward and backward passes and grasp the intuition behind backpropagation.

- `1b_Fully_Connected_ASL_Dataset.ipynb`  
  â¤ Apply your fully connected model to the **American Sign Language (ASL)** image dataset. Explore preprocessing, data handling, and performance tuning.

---

### ğŸ”¹ Module 2: Deep Learning with PyTorch

- `2a_FC_ASL_using_pytorch.ipynb`  
  â¤ Transition from NumPy to **PyTorch**. Rebuild and train a fully connected model for the ASL dataset using high-level PyTorch APIs.

- `2b_CNN_Cifar_10_Using_Pytorch.ipynb`  
  â¤ Dive into **Convolutional Neural Networks (CNNs)** and apply them to the **CIFAR-10** image dataset. Understand how spatial hierarchies improve classification tasks.

- `2c_Transfer_Learning_CIFAR-10.ipynb`  
  â¤ Learn how to leverage pre-trained models through **transfer learning**, fine-tuning them on CIFAR-10 for faster convergence and better accuracy.

---

### ğŸ”¹ Module 3: Embeddings and Sequential Models

- `3a_Embeddings with Glove and Numpy.ipynb`  
  â¤ Use **GloVe** pre-trained word vectors to build meaningful word embeddings with NumPy. Understand the semantic relationships captured through vector space.

- `3b_Text_Classification_with RNNs and AG_News.ipynb`  
  â¤ Implement **Recurrent Neural Networks (RNNs)** for text classification on the **AG News** dataset. Learn about sequence modeling and temporal dependencies in language.

---

### ğŸ”¹ Module 4: Transformers and Advanced Architectures

- `4_Text_Translator_Using_Transformers.ipynb`  
  â¤ Explore **Transformer architectures** and build a simple text translator using attention mechanisms â€” the foundation of todayâ€™s state-of-the-art NLP systems.

- `Paper_Transformers Beyond NLP.pdf`  
  â¤ Bonus reading: A comprehensive paper on how **Transformers** extend beyond NLP, impacting vision, speech, and multimodal tasks.

---

## ğŸ§ª Who Is This For?

This repository is ideal for:

- ğŸ‘©â€ğŸ’» Beginners seeking an end-to-end learning path from scratch to state-of-the-art models.
- ğŸ§‘â€ğŸ« Educators or students exploring step-by-step implementations.
- ğŸ› ï¸ Practitioners wanting to bridge theoretical foundations with practical PyTorch usage.

---

## ğŸ“ Repository Structure

```bash
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1a_Multilayer_Fully_Connected_Numpy.ipynb
â”‚   â”œâ”€â”€ 1b_Fully_Connected_ASL_Dataset.ipynb
â”‚   â”œâ”€â”€ 2a_FC_ASL_using_pytorch.ipynb
â”‚   â”œâ”€â”€ 2b_CNN_Cifar_10_Using_Pytorch.ipynb
â”‚   â”œâ”€â”€ 2c_Transfer_Learning_CIFAR-10.ipynb
â”‚   â”œâ”€â”€ 3a_Embeddings with Glove and Numpy.ipynb
â”‚   â”œâ”€â”€ 3b_Text_Classification_with RNNs and AG_News.ipynb
â”‚   â”œâ”€â”€ 4_Text_Translator_Using_Transformers.ipynb
â”‚   â””â”€â”€ Paper_Transformers Beyond NLP.pdf
â”œâ”€â”€ images/
â”‚   â””â”€â”€ (Visual aids and diagrams)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```
## ğŸš€ Getting Started

1.-Clone this repository:
git clone https://github.com/Alvarez527/Advanced-ML.git
cd Advanced-ML

2.-(Optional) Create a virtual environment:
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

3.-Install required packages:
pip install -r requirements.txt

4.-Launch Jupyter:
jupyter notebook

## ğŸ¯ Learning Goals
By the end of this journey, you will be able to:

Grasp the architecture and math behind fully connected and convolutional networks.

Build and train models using NumPy and PyTorch.

Understand and apply word embeddings and RNNs to text data.

Use transfer learning and transformers for cutting-edge performance.

## ğŸ“ Recommended Resources
Deep Learning Book by Goodfellow, Bengio & Courville

Stanford CS231n: CNNs for Visual Recognition

The Illustrated Transformer

PyTorch Documentation

## ğŸ¤ Contributing
Have ideas, improvements, or corrections? Feel free to fork, submit issues, or open a pull request. Let's build this knowledge base together!





